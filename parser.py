import asyncio
import random
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from supabase import create_client

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Supabase
URL = "https://qenfvaqhbbqmorhvwmxg.supabase.co"
KEY = "sb_secret_JCr9KEjlFpMBO3fymu5l-w_AHPa7qac" 
supabase = create_client(URL, KEY)

async def human_delay():
    """–ò–º–∏—Ç–∞—Ü–∏—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –ø–∞—É–∑—ã"""
    await asyncio.sleep(random.uniform(2, 5))

async def scrape_car_page(context, url):
    page = await context.new_page()
    try:
        # –≠–º—É–ª—è—Ü–∏—è –∑–∞—Ö–æ–¥–∞ —á–µ–ª–æ–≤–µ–∫–∞
        await page.goto(url, wait_until="networkidle", timeout=90000)
        await human_delay()
        
        # –ü—Ä–æ–∫—Ä—É—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤–Ω–∏–∑, —á—Ç–æ–±—ã –ø–æ–¥–≥—Ä—É–∑–∏–ª–∏—Å—å –ª–µ–Ω–∏–≤—ã–µ —Ñ–æ—Ç–æ
        await page.evaluate("window.scrollTo(0, document.body.scrollHeight/2)")
        await human_delay()

        soup = BeautifulSoup(await page.content(), 'html.parser')
        
        # 1. –ó–∞–≥–æ–ª–æ–≤–æ–∫
        title = soup.select_one('h1.entry-title').get_text(strip=True) if soup.select_one('h1.entry-title') else "Unknown Model"
        
        # 2. –í—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–æ—Ç–æ (–¥–µ–ª–∞–µ–º –º–∞—Å—Å–∏–≤ –¥–ª—è –≥–∞–ª–µ—Ä–µ–∏)
        all_imgs = []
        for img in soup.find_all('img', src=True):
            src = img['src']
            if 'uploads' in src and 'logo' not in src.lower():
                all_imgs.append(src)
        
        # –û—Å–Ω–æ–≤–Ω–æ–µ —Ñ–æ—Ç–æ ‚Äî –ø–µ—Ä–≤–æ–µ –∏–∑ —Å–ø–∏—Å–∫–∞
        main_image = all_imgs[0] if all_imgs else ""
        
        # 3. –û–ø–∏—Å–∞–Ω–∏–µ (–∏—â–µ–º —Ç–µ–∫—Å—Ç –ª–æ—Ç–∞)
        desc_box = soup.select_one('.elementor-text-editor') or soup.select_one('.entry-content')
        description = desc_box.get_text(separator=' ', strip=True)[:500] if desc_box else "Premium stock car."

        car_data = {
            "external_id": url.strip('/').split('/')[-1],
            "brand_model": title.upper(),
            "description": description,
            "image_url": main_image,
            "source_url": url,
            # –î–æ–ø. –ø–æ–ª–µ, –µ—Å–ª–∏ —Ç—ã –¥–æ–±–∞–≤–∏–ª –µ–≥–æ –≤ –±–∞–∑—É –¥–ª—è –≥–∞–ª–µ—Ä–µ–∏
            "gallery": all_imgs[:5] 
        }
        
        supabase.table("cars").upsert(car_data).execute()
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω –ª–æ—Ç: {title}")

    except Exception as e:
        print(f"‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞ —Å {url}: {e}")
    finally:
        await page.close()

async def run_parser():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True) # –°—Ç–∞–≤–∏–º headless=True –¥–ª—è GitHub
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            viewport={'width': 1920, 'height': 1080}
        )
        page = await context.new_page()
        
        print("üåê –ó–∞—Ö–æ–¥–∏–º –Ω–∞ –≥–ª–∞–≤–Ω—É—é...")
        await page.goto("https://rbautotrade.com/", wait_until="networkidle")
        await human_delay()
        
        # –ò—â–µ–º –∫–Ω–æ–ø–∫—É Car Listing –∏ –∫–ª–∏–∫–∞–µ–º
        try:
            await page.click("text=CAR LISTING", timeout=10000)
            print("üìÇ –ü–µ—Ä–µ—à–ª–∏ –≤ —Ä–∞–∑–¥–µ–ª Car Listing")
        except:
            await page.goto("https://rbautotrade.com/inventory/", wait_until="networkidle")
        
        await human_delay()
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–∞—Ä—Ç–æ—á–∫–∏
        soup = BeautifulSoup(await page.content(), 'html.parser')
        links = []
        for a in soup.find_all('a', href=True):
            if '/inventory/' in a['href'] and a['href'] != "https://rbautotrade.com/inventory/":
                links.append(a['href'])
        
        unique_links = list(set(links))
        print(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(unique_links)} –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π. –ù–∞—á–∏–Ω–∞–µ–º —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö...")
        
        for link in unique_links[:15]: # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 15 –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö
            await scrape_car_page(context, link)
            
        await browser.close()

if __name__ == "__main__":
    asyncio.run(run_parser())