import requests
from bs4 import BeautifulSoup
from supabase import create_client
import time

# –î–∞–Ω–Ω—ã–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
URL = "https://qenfvaqhbbqmorhvwmxg.supabase.co"
KEY = "sb_secret_JCr9KEjlFpMBO3fymu5l-w_AHPa7qac" 
supabase = create_client(URL, KEY)

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8"
}

def scrape():
    print("üöÄ –°—Ç–∞—Ä—Ç –≥–ª—É–±–æ–∫–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞...")
    try:
        # 1. –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏–Ω–≤–µ–Ω—Ç–∞—Ä—è
        response = requests.get("https://rbautotrade.com/inventory/", headers=HEADERS, timeout=30)
        print(f"üì° –°—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞ —Å–∞–π—Ç–∞: {response.status_code}")
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # –°–æ–±–∏—Ä–∞–µ–º –í–°–ï —Å—Å—ã–ª–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç —Å–ª–æ–≤–æ 'inventory'
        links = []
        for a in soup.find_all('a', href=True):
            href = a['href']
            # –§–∏–ª—å—Ç—Ä—É–µ–º: —Å—Å—ã–ª–∫–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –¥–ª–∏–Ω–Ω–æ–π (–∑–Ω–∞—á–∏—Ç —ç—Ç–æ –ª–æ—Ç) –∏ –Ω–µ –±—ã—Ç—å —Å–∞–º–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ–π –∏–Ω–≤–µ–Ω—Ç–∞—Ä—è
            if '/inventory/' in href and len(href) > 35:
                links.append(href)
        
        unique_links = list(set(links))
        print(f"üîé –ù–∞–π–¥–µ–Ω–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –ª–æ—Ç–æ–≤: {len(unique_links)}")

        if not unique_links:
            print("‚ùå –°—Å—ã–ª–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –í–æ–∑–º–æ–∂–Ω–æ, —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–∞–π—Ç–∞ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å –∏–ª–∏ –¥–æ—Å—Ç—É–ø –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω.")
            # –î–ª—è —Ç–µ—Å—Ç–∞: –≤—ã–≤–µ–¥–µ–º —á–∞—Å—Ç—å HTML, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å —á—Ç–æ –≤–∏–¥–∏—Ç –±–æ—Ç
            print("DEBUG: –ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü—ã:", response.text[:500])
            return

        for link in unique_links[:15]:
            try:
                print(f"üïµÔ∏è –ü–∞—Ä—Å–∏–º –ª–æ—Ç: {link}")
                res = requests.get(link, headers=HEADERS, timeout=30)
                car_soup = BeautifulSoup(res.text, 'html.parser')
                
                # –ó–∞–≥–æ–ª–æ–≤–æ–∫
                title = "Unknown Car"
                h1 = car_soup.find('h1')
                if h1:
                    title = h1.get_text(strip=True).upper()
                
                # –§–æ—Ç–æ
                images = []
                for img in car_soup.find_all('img', src=True):
                    src = img['src']
                    if 'uploads' in src and 'logo' not in src.lower() and 'thumb' not in src.lower():
                        images.append(src)
                
                main_img = images[0] if images else ""
                
                # –û–ø–∏—Å–∞–Ω–∏–µ
                desc_tag = car_soup.select_one('.entry-content') or car_soup.select_one('.elementor-widget-theme-post-content')
                description = desc_tag.get_text(separator=' ', strip=True) if desc_tag else "No description"

                car_data = {
                    "external_id": link.strip('/').split('/')[-1],
                    "brand_model": title,
                    "description": description[:1000],
                    "image_url": main_img,
                    "source_url": link
                }

                # –ó–ê–ü–ò–°–¨ –í –ë–ê–ó–£
                result = supabase.table("cars").upsert(car_data).execute()
                print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ Supabase: {title}")
                
            except Exception as car_e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –Ω–∞ –ª–æ—Ç–µ {link}: {car_e}")
            
            time.sleep(2) # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏

    except Exception as e:
        print(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    scrape()