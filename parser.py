import requests
from bs4 import BeautifulSoup
from supabase import create_client
import time

# –¢–≤–æ–∏ –¥–∞–Ω–Ω—ã–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
URL = "https://qenfvaqhbbqmorhvwmxg.supabase.co"
KEY = "sb_secret_JCr9KEjlFpMBO3fymu5l-w_AHPa7qac" 
supabase = create_client(URL, KEY)

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
}

def scrape():
    print("üöÄ –ó–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞...")
    
    # 1. –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ –º–∞—à–∏–Ω—ã –Ω–∞–ø—Ä—è–º—É—é –∏–∑ HTML
    try:
        response = requests.get("https://rbautotrade.com/inventory/", headers=HEADERS, timeout=30)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–∞—Ä—Ç–æ—á–∫–∏
        links = []
        for a in soup.find_all('a', href=True):
            href = a['href']
            if '/inventory/' in href and href.count('/') > 4:
                links.append(href)
        
        unique_links = list(set(links))
        print(f"üîé –ù–∞–π–¥–µ–Ω–æ –º–∞—à–∏–Ω –Ω–∞ —Å–∞–π—Ç–µ: {len(unique_links)}")

        if not unique_links:
            print("‚ùå –°–∞–π—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –¥–æ—Å—Ç—É–ø. –ü—Ä–æ–±—É–µ–º –∑–∞–ø–∞—Å–Ω–æ–π –º–µ—Ç–æ–¥...")
            return

        for link in unique_links[:15]:
            print(f"üïµÔ∏è –û–±—Ä–∞–±–æ—Ç–∫–∞: {link}")
            res = requests.get(link, headers=HEADERS, timeout=30)
            car_soup = BeautifulSoup(res.text, 'html.parser')
            
            # –ù–∞–∑–≤–∞–Ω–∏–µ
            title = car_soup.find('h1').get_text(strip=True).upper() if car_soup.find('h1') else "AUTO KOREA"
            
            # –§–æ—Ç–æ (—Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏ –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞)
            images = []
            for img in car_soup.find_all('img', src=True):
                src = img['src']
                if 'uploads' in src and 'logo' not in src.lower():
                    images.append(src)
            
            main_img = images[0] if images else ""
            
            # –û–ø–∏—Å–∞–Ω–∏–µ
            desc_tag = car_soup.select_one('.entry-content') or car_soup.select_one('.elementor-widget-theme-post-content')
            description = desc_tag.get_text(separator=' ', strip=True) if desc_tag else "No description available"

            car_data = {
                "external_id": link.strip('/').split('/')[-1],
                "brand_model": title,
                "description": description[:1000],
                "image_url": main_img,
                "source_url": link
            }

            # –ó–∞–ø–∏—Å—å –≤ Supabase
            supabase.table("cars").upsert(car_data).execute()
            print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {title}")
            time.sleep(1) # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞, —á—Ç–æ–±—ã –Ω–µ –∑–∞–±–∞–Ω–∏–ª–∏

    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    scrape()