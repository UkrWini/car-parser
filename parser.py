import os
import asyncio
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from supabase import create_client

# –î–∞–Ω–Ω—ã–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
URL = os.environ.get("SUPABASE_URL", "https://qenfvaqhbbqmorhvwmxg.supabase.co")
KEY = os.environ.get("SUPABASE_KEY", "sb_secret_JCr9KEjlFpMBO3fymu5l-w_AHPa7qac")
supabase = create_client(URL, KEY)

async def scrape_details(context, url):
    # –£–ª—É—á—à–µ–Ω–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä –º—É—Å–æ—Ä–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
    blacklist = ["search=", "how-to-buy", "hot-deals", "stand-by", "auction", "car-listings"]
    if any(x in url for x in blacklist): return 
    
    page = await context.new_page()
    try:
        # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —á—É—Ç—å –¥–æ–ª—å—à–µ –¥–ª—è –∫–∞—Ä—Ç–∏–Ω–æ–∫
        await page.goto(url, wait_until="domcontentloaded", timeout=60000)
        await asyncio.sleep(3) 
        
        soup = BeautifulSoup(await page.content(), 'html.parser')
        
        # 1. –¢–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        title_tag = soup.select_one('h1.entry-title') or soup.find('h1')
        title = title_tag.get_text(strip=True).upper() if title_tag else "UNKNOWN CAR"

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–¥–∞–Ω–Ω—ã–µ –∞–≤—Ç–æ
        if any(word in title for word in ["SOLD", "–ü–†–û–î–ê–ù–û", "RESERVED"]): return

        # 2. –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –≥–ª–∞–≤–Ω–æ–≥–æ –§–û–¢–û
        # –ò—â–µ–º —Å–Ω–∞—á–∞–ª–∞ –ø–æ –∫–ª–∞—Å—Å—É –æ–±–ª–æ–∂–∫–∏, –ø–æ—Ç–æ–º –≤ —Å–ª–∞–π–¥–µ—Ä–µ, –ø–æ—Ç–æ–º –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–≤—É—é –∫—Ä—É–ø–Ω—É—é –∫–∞—Ä—Ç–∏–Ω–∫—É
        img_tag = (
            soup.select_one('img.wp-post-image') or 
            soup.select_one('.elementor-image img') or 
            soup.select_one('.attachment-full') or
            soup.find('img', src=lambda x: x and 'uploads' in x)
        )
        
        img_url = ""
        if img_tag:
            img_url = img_tag.get('src') or img_tag.get('data-src') or ""
        
        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏
        if img_url and img_url.startswith('/'):
            img_url = "https://rbautotrade.com" + img_url

        car_data = {
            "external_id": url.split('/')[-1].split('?')[0] or "id_" + title[:10],
            "brand_model": title,
            "description": "Verified premium vehicle from South Korea",
            "image_url": img_url,
            "source_url": url
        }
        
        # –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ –±–∞–∑—É
        supabase.table("cars").upsert(car_data).execute()
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {title}")

    except Exception as e: 
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –Ω–∞ {url}: {e}")
    finally: 
        await page.close()

async def run_parser():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(user_agent="Mozilla/5.0")
        page = await context.new_page()
        
        print("üîó –ó–∞—Ö–æ–¥–∏–º –≤ –∏–Ω–≤–µ–Ω—Ç–∞—Ä—å...")
        await page.goto("https://rbautotrade.com/inventory/", wait_until="networkidle")
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–∞—Ä—Ç–æ—á–∫–∏ –º–∞—à–∏–Ω
        content = await page.content()
        soup = BeautifulSoup(content, 'html.parser')
        links = [a['href'] for a in soup.select('a[href*="/inventory/"]')]
        unique_links = list(set(links))
        
        print(f"üîé –ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤—ã—Ö –∞–≤—Ç–æ: {len(unique_links)}")
        
        # –ü–∞—Ä—Å–∏–º –ø–µ—Ä–≤—ã–µ 20 –º–∞—à–∏–Ω
        for link in unique_links[:20]:
            await scrape_details(context, link)
            
        await browser.close()

if __name__ == "__main__":
    asyncio.run(run_parser())