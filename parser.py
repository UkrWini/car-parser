import asyncio
import random
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from supabase import create_client

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ —Ç–≤–æ–µ–π –±–∞–∑–µ
URL = "https://qenfvaqhbbqmorhvwmxg.supabase.co"
KEY = "sb_secret_JCr9KEjlFpMBO3fymu5l-w_AHPa7qac" 
supabase = create_client(URL, KEY)

async def scrape_car_details(context, url):
    page = await context.new_page()
    try:
        print(f"üïµÔ∏è –ó–∞—Ö–æ–∂—É –≤ –º–∞—à–∏–Ω—É: {url}")
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –∂–¥–µ–º, –ø–æ–∫–∞ –æ–Ω–∞ —Ä–µ–∞–ª—å–Ω–æ –ø—Ä–æ–≥—Ä—É–∑–∏—Ç—Å—è
        await page.goto(url, wait_until="domcontentloaded", timeout=60000)
        await asyncio.sleep(random.uniform(5, 8)) # –î–∞–µ–º –≤—Ä–µ–º—è —Å–∫—Ä–∏–ø—Ç–∞–º —Å–∞–π—Ç–∞
        
        # –õ–∏—Å—Ç–∞–µ–º –≤–Ω–∏–∑, —á—Ç–æ–±—ã —Å—Ä–∞–±–æ—Ç–∞–ª–∏ –ª–µ–Ω–∏–≤—ã–µ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–æ—Ç–æ
        await page.mouse.wheel(0, 2000)
        await asyncio.sleep(2)

        content = await page.content()
        soup = BeautifulSoup(content, 'html.parser')

        # 1. –ó–∞–≥–æ–ª–æ–≤–æ–∫
        title = soup.find('h1').get_text(strip=True).upper() if soup.find('h1') else "KOREAN CAR"
        
        # 2. –í–°–ï —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ (–¥–ª—è –≥–∞–ª–µ—Ä–µ–∏)
        images = []
        for img in soup.find_all('img', src=True):
            src = img['src']
            if 'uploads' in src and 'logo' not in src.lower() and 'thumb' not in src.lower():
                images.append(src)
        
        unique_images = list(dict.fromkeys(images))
        main_image = unique_images[0] if unique_images else ""
        
        # 3. –í–∏–¥–µ–æ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        video_url = ""
        iframe = soup.find('iframe', src=True)
        if iframe and 'youtube' in iframe['src']:
            video_url = iframe['src']

        # 4. –ü–æ–ª–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ª–æ—Ç–∞
        desc_box = soup.select_one('.elementor-widget-theme-post-content') or soup.select_one('.entry-content')
        description = desc_box.get_text(separator='\n', strip=True) if desc_box else ""

        car_data = {
            "external_id": url.strip('/').split('/')[-1],
            "brand_model": title,
            "description": description[:2000],
            "image_url": main_image,
            "source_url": url
            # –ï—Å–ª–∏ –¥–æ–±–∞–≤–∏—à—å –≤ –±–∞–∑—É –∫–æ–ª–æ–Ω–∫—É video_url, —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π –Ω–∏–∂–µ:
            # "video_url": video_url 
        }

        supabase.table("cars").upsert(car_data).execute()
        print(f"‚úÖ –ü–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {title}")

    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ –∫–∞—Ä—Ç–æ—á–∫–µ {url}: {e}")
    finally:
        await page.close()

async def run_parser():
    async with async_playwright() as p:
        # –ó–∞–ø—É—Å–∫ "–Ω–µ–≤–∏–¥–∏–º–æ–≥–æ" –±—Ä–∞—É–∑–µ—Ä–∞
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            viewport={'width': 1920, 'height': 1080},
            device_scale_factor=1,
        )
        
        page = await context.new_page()
        print("üåê –ó–∞—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏–Ω–≤–µ–Ω—Ç–∞—Ä—è –Ω–∞–ø—Ä—è–º—É—é...")
        
        try:
            # –ü–µ—Ä–µ—Ö–æ–¥–∏–º —Å—Ä–∞–∑—É –Ω–∞ —Å–ø–∏—Å–æ–∫ –º–∞—à–∏–Ω
            await page.goto("https://rbautotrade.com/inventory/", wait_until="networkidle", timeout=60000)
            await asyncio.sleep(5)
            
            # –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –≤—Å–µ –º–∞—à–∏–Ω—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
            links = await page.eval_on_selector_all(
                "a[href*='/inventory/']", 
                "elements => elements.map(e => e.href)"
            )
            
            # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏ –¥–ª–∏–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ (–∫–∞—Ä—Ç–æ—á–∫–∏)
            unique_links = list(set([l for l in links if len(l) > 40]))
            print(f"üîé –ù–∞–π–¥–µ–Ω–æ –º–∞—à–∏–Ω –≤ –ª–∏—Å—Ç–∏–Ω–≥–µ: {len(unique_links)}")

            for link in unique_links[:12]: # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 12 –¥–ª—è –Ω–∞—á–∞–ª–∞
                await scrape_car_details(context, link)
                await asyncio.sleep(random.uniform(3, 6)) # –ü–∞—É–∑–∞ –∫–∞–∫ —É —á–µ–ª–æ–≤–µ–∫–∞

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞ –≥–ª–∞–≤–Ω–æ–π: {e}")
        
        await browser.close()

if __name__ == "__main__":
    asyncio.run(run_parser())