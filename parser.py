import os
import asyncio
import random
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from supabase import create_client

# --- –ù–ê–°–¢–†–û–ô–ö–ò SUPABASE ---
# –ö–æ–¥ —Å–Ω–∞—á–∞–ª–∞ –∏—â–µ—Ç –∫–ª—é—á–∏ –≤ —Å–µ–∫—Ä–µ—Ç–∞—Ö GitHub, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç ‚Äî –±–µ—Ä–µ—Ç –ª–æ–∫–∞–ª—å–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∞
URL = os.environ.get("SUPABASE_URL")
KEY = os.environ.get("SUPABASE_KEY")

if not URL or not KEY:
    URL = "https://qenfvaqhbbqmorhvwmxg.supabase.co"
    KEY = "sb_secret_JCr9KEjlFpMBO3fymu5l-w_AHPa7qac"

supabase = create_client(URL, KEY)

async def scrape_details(context, url):
    """–ó–∞—Ö–æ–¥–∏—Ç –≤–Ω—É—Ç—Ä—å –∫–∞–∂–¥–æ–π –º–∞—à–∏–Ω—ã –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç–∞—Ç—É—Å"""
    page = await context.new_page()
    try:
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –≤ –æ–±–ª–∞–∫–µ
        await page.goto(url, wait_until="domcontentloaded", timeout=60000)
        await asyncio.sleep(random.uniform(2, 4))

        soup = BeautifulSoup(await page.content(), 'html.parser')
        
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫
        title_tag = soup.find(['h1', 'h4'])
        title = title_tag.get_text(strip=True).upper() if title_tag else "–ë–ï–ó –ù–ê–ó–í–ê–ù–ò–Ø"

        # –ü–†–û–í–ï–†–ö–ê –ù–ê –ü–†–û–î–ê–ù–û
        if "[SOLDOUT]" in title or "SOLD" in title:
            print(f"‚è© –ü–†–û–ü–£–°–ö: {title} (–ü—Ä–æ–¥–∞–Ω–æ)")
            return None

        # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –±–∞–∑—ã
        description = soup.find('div', class_='entry-content')
        desc_text = description.get_text(strip=True)[:500] if description else ""
        
        imgs = [img['src'] for img in soup.find_all('img') if 'cdn' in img.get('src', '')]
        main_img = imgs[0] if imgs else ""

        car_data = {
            "external_id": url.split('/')[-1].split('?')[0],
            "brand_model": title,
            "description": desc_text,
            "image_url": main_img,
            "source_url": url
        }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Supabase
        try:
            supabase.table("cars").upsert(car_data).execute()
            print(f"‚úÖ –î–û–ë–ê–í–õ–ï–ù–û: {title}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –±–∞–∑—ã: {e}")

    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {url}: {e}")
    finally:
        await page.close()

async def run_parser():
    async with async_playwright() as p:
        # –í–ê–ñ–ù–û: headless=True –¥–ª—è —Ä–∞–±–æ—Ç—ã –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ GitHub
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(user_agent="Mozilla/5.0...")
        page = await context.new_page()

        print("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞...")
        await page.goto("https://rbautotrade.com/inventory/", wait_until="networkidle")

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –º–∞—à–∏–Ω—ã
        content = await page.content()
        soup = BeautifulSoup(content, 'html.parser')
        links = [a['href'] for a in soup.select('a[href*="/inventory/"]')]
        unique_links = list(set(links))

        print(f"üîé –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {len(unique_links)}")

        for link in unique_links:
            await scrape_details(context, link)

        await browser.close()
        print("üéâ –í–°–Å! –ü—Ä–æ–≤–µ—Ä—å –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö.")

if __name__ == "__main__":
    asyncio.run(run_parser())