import os
import asyncio
import random
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from supabase import create_client

# –î–∞–Ω–Ω—ã–µ Supabase
URL = os.environ.get("SUPABASE_URL")
KEY = os.environ.get("SUPABASE_KEY")
if not URL or not KEY:
    URL = "https://qenfvaqhbbqmorhvwmxg.supabase.co"
    KEY = "sb_secret_JCr9KEjlFpMBO3fymu5l-w_AHPa7qac"
supabase = create_client(URL, KEY)

async def scrape_details(context, url):
    """–ó–∞—Ö–æ–¥–∏—Ç –≤–Ω—É—Ç—Ä—å –∫–∞–∂–¥–æ–π –º–∞—à–∏–Ω—ã"""
    page = await context.new_page()
    try:
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç, —Ç–∞–∫ –∫–∞–∫ —Å–∞–π—Ç –º–æ–∂–µ—Ç –ø–æ–¥—Ç–æ—Ä–º–∞–∂–∏–≤–∞—Ç—å
        await page.goto(url, wait_until="domcontentloaded", timeout=60000)
        await asyncio.sleep(2)
        
        soup = BeautifulSoup(await page.content(), 'html.parser')
        title = soup.find(['h1', 'h4']).get_text(strip=True).upper() if soup.find(['h1', 'h4']) else ""
        
        # –ì–õ–ê–í–ù–û–ï –£–°–õ–û–í–ò–ï: –ü—Ä–æ–ø—É—Å–∫ –ø—Ä–æ–¥–∞–Ω–Ω—ã—Ö
        if "SOLD" in title or "RESERVED" in title:
            print(f"   ‚è© –ü–†–û–ü–£–°–ö: {title[:30]}... (–ü—Ä–æ–¥–∞–Ω–æ)")
            return

        # –ï—Å–ª–∏ –Ω–µ –ø—Ä–æ–¥–∞–Ω–æ - —Å–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        imgs = [i['src'] for i in soup.find_all('img', src=True) if 'post' in i['src']]
        
        car_data = {
            "external_id": url.split('/')[-1].split('?')[0],
            "brand_model": title.title(),
            "image_url": imgs[0] if imgs else "",
            "source_site": "rbautotrade.com"
        }
        
        supabase.table("cars").upsert(car_data, on_conflict="external_id").execute()
        print(f"   ‚úÖ –î–û–ë–ê–í–õ–ï–ù–û: {title[:30]}")

    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –≤ –∫–∞—Ä—Ç–æ—á–∫–µ: {url[-10:]}...")
    finally:
        await page.close()

async def run_parser():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True) # –í–∏–¥–∏–º –ø—Ä–æ—Ü–µ—Å—Å
        context = await browser.new_context(user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36")
        page = await context.new_page()

        for cp in range(1, 11):
            url = f"https://rbautotrade.com/posts/car-listings?page={cp}"
            print(f"\nüìÇ –°–¢–†–ê–ù–ò–¶–ê {cp} -------------------------")
            
            try:
                await page.goto(url, wait_until="load", timeout=60000)
                await asyncio.sleep(4)
                
                # –°–æ–±–∏—Ä–∞–µ–º –í–°–ï —Å—Å—ã–ª–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –≤–µ–¥—É—Ç –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
                hrefs = await page.eval_on_selector_all("a", "elements => elements.map(e => e.href)")
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º: —Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ —Å–æ–¥–µ—Ä–∂–∞—Ç /car-listings/ –∏ –∏–º–µ—é—Ç ID –≤ –∫–æ–Ω—Ü–µ
                valid_urls = list(set([h for h in hrefs if "/posts/car-listings/" in h and any(char.isdigit() for char in h.split('/')[-1])]))
                
                print(f"üìä –ù–∞–π–¥–µ–Ω–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –º–∞—à–∏–Ω: {len(valid_urls)}")

                # –ò–¥–µ–º –ø–æ –∫–∞–∂–¥–æ–π —Å—Å—ã–ª–∫–µ
                for car_url in valid_urls:
                    if car_url.strip('/').endswith('car-listings'): continue
                    await scrape_details(context, car_url)
                    await asyncio.sleep(random.uniform(1, 2))

            except Exception as e:
                print(f"‚ö†Ô∏è –°—Ç—Ä–∞–Ω–∏—Ü–∞ {cp} –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å. –ò–¥—É –¥–∞–ª—å—à–µ.")

        await browser.close()
        print("\nüéâ –í–°–Å! –ü—Ä–æ–≤–µ—Ä—å –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö.")

if __name__ == "__main__":
    asyncio.run(run_parser())