import os
import asyncio
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from supabase import create_client

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
URL = os.environ.get("SUPABASE_URL", "https://qenfvaqhbbqmorhvwmxg.supabase.co")
KEY = os.environ.get("SUPABASE_KEY", "sb_secret_JCr9KEjlFpMBO3fymu5l-w_AHPa7qac")
supabase = create_client(URL, KEY)

async def scrape_details(context, url):
    # –£–ª—É—á—à–µ–Ω–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä –º—É—Å–æ—Ä–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
    blacklist = ["search=", "how-to-buy", "hot-deals", "stand-by", "auction", "car-listings"]
    if any(x in url for x in blacklist): return 
    
    page = await context.new_page()
    try:
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –¥–ª—è –ø–æ–¥–≥—Ä—É–∑–∫–∏ —Ñ–æ—Ç–æ
        await page.goto(url, wait_until="domcontentloaded", timeout=60000)
        await asyncio.sleep(4) # –î–∞–µ–º JS –ø–æ–¥–≥—Ä—É–∑–∏—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫–∏
        
        soup = BeautifulSoup(await page.content(), 'html.parser')
        
        # –ü–æ–∏—Å–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        title_tag = soup.select_one('h1.entry-title') or soup.find('h1')
        title = title_tag.get_text(strip=True).upper() if title_tag else "UNKNOWN CAR"

        if any(word in title for word in ["SOLD", "–ü–†–û–î–ê–ù–û", "RESERVED"]): return

        # –ú–û–©–ù–´–ô –ü–û–ò–°–ö –ö–ê–†–¢–ò–ù–ö–ò (—Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è rbautotrade.com)
        img_url = ""
        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ—Å—Ç–∞ –∏–ª–∏ –ø–µ—Ä–≤–æ–µ –∏–∑ –≥–∞–ª–µ—Ä–µ–∏
        img_tag = (
            soup.select_one('img.wp-post-image') or 
            soup.select_one('.elementor-image img') or 
            soup.select_one('.slick-active img') or
            soup.find('img', src=lambda x: x and 'uploads' in x and 'logo' not in x.lower())
        )
        
        if img_tag:
            img_url = img_tag.get('src') or img_tag.get('data-src') or ""
        
        if img_url and img_url.startswith('/'):
            img_url = "https://rbautotrade.com" + img_url

        car_data = {
            "external_id": url.split('/')[-1].split('?')[0] or f"id_{hash(title)}",
            "brand_model": title,
            "description": "Premium vehicle from South Korea. Verified condition.",
            "image_url": img_url,
            "source_url": url
        }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É
        supabase.table("cars").upsert(car_data).execute()
        print(f"‚úÖ –ì–æ—Ç–æ–≤–æ: {title}")

    except Exception as e: 
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –Ω–∞ {url}: {e}")
    finally: 
        await page.close()

async def run_parser():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        # –ú–∞—Å–∫–∏—Ä—É–µ–º—Å—è –ø–æ–¥ –æ–±—ã—á–Ω—ã–π –±—Ä–∞—É–∑–µ—Ä
        context = await browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
        page = await context.new_page()
        
        print("üîó –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–≤–µ–Ω—Ç–∞—Ä—è...")
        await page.goto("https://rbautotrade.com/inventory/", wait_until="networkidle")
        
        links = [a['href'] for a in (BeautifulSoup(await page.content(), 'html.parser')).select('a[href*="/inventory/"]')]
        unique_links = list(set(links))
        
        print(f"üîé –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {len(unique_links)}. –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –ø–µ—Ä–≤—ã—Ö 15...")
        
        for link in unique_links[:15]:
            await scrape_details(context, link)
            
        await browser.close()

if __name__ == "__main__":
    asyncio.run(run_parser())