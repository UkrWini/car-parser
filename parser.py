import asyncio
import random
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from supabase import create_client

# –¢–≤–æ–∏ –¥–∞–Ω–Ω—ã–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
URL = "https://qenfvaqhbbqmorhvwmxg.supabase.co"
KEY = "sb_secret_JCr9KEjlFpMBO3fymu5l-w_AHPa7qac" 
supabase = create_client(URL, KEY)

async def human_behavior(page):
    """–ò–º–∏—Ç–∞—Ü–∏—è —Ö–∞–æ—Ç–∏—á–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è —á–µ–ª–æ–≤–µ–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ"""
    for _ in range(random.randint(3, 6)):
        await page.mouse.wheel(0, random.randint(300, 700))
        await asyncio.sleep(random.uniform(1.0, 2.5))

async def scrape_car_details(context, url):
    page = await context.new_page()
    try:
        # –ó–∞—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–∞—à–∏–Ω—ã
        print(f"üïµÔ∏è –ó–∞—Ö–æ–¥–∏–º –≤ –ª–æ—Ç: {url}")
        await page.goto(url, wait_until="networkidle", timeout=90000)
        await human_behavior(page) # –õ–∏—Å—Ç–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É, —á—Ç–æ–±—ã –ø–æ–¥–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç

        content = await page.content()
        soup = BeautifulSoup(content, 'html.parser')

        # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        title = soup.select_one('h1.entry-title').get_text(strip=True).upper() if soup.select_one('h1.entry-title') else "UNKNOWN MODEL"
        
        # –°–æ–±–∏—Ä–∞–µ–º –í–°–ï —Ñ–æ—Ç–æ –¥–ª—è –≥–∞–ª–µ—Ä–µ–∏
        images = []
        for img in soup.find_all('img', src=True):
            src = img['src']
            if 'uploads' in src and 'logo' not in src.lower() and 'icon' not in src.lower():
                images.append(src)
        
        unique_images = list(dict.fromkeys(images)) # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        main_img = unique_images[0] if unique_images else ""

        # –í—ã—Ç—è–≥–∏–≤–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
        desc_element = soup.select_one('.elementor-widget-theme-post-content') or soup.select_one('.entry-content')
        description = desc_element.get_text(separator='\n', strip=True) if desc_element else "Premium Korea Stock"

        car_data = {
            "external_id": url.strip('/').split('/')[-1],
            "brand_model": title,
            "description": description[:1000], # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            "image_url": main_img,
            "source_url": url
        }

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º/–û–±–Ω–æ–≤–ª—è–µ–º –≤ –±–∞–∑–µ
        supabase.table("cars").upsert(car_data).execute()
        print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –ª–æ—Ç–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {title}")

    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –ª–æ—Ç–∞ {url}: {e}")
    finally:
        await page.close()

async def run_parser():
    async with async_playwright() as p:
        # –ó–∞–ø—É—Å–∫–∞–µ–º –±—Ä–∞—É–∑–µ—Ä —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ —Å–∫—Ä—ã—Ç–Ω–æ—Å—Ç–∏
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            viewport={'width': 1920, 'height': 1080},
            extra_http_headers={"Accept-Language": "en-US,en;q=0.9"}
        )
        page = await context.new_page()

        print("üåê –û—Ç–∫—Ä—ã–≤–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É...")
        await page.goto("https://rbautotrade.com/", wait_until="networkidle")
        await asyncio.sleep(random.uniform(3, 5))

        # –í–º–µ—Å—Ç–æ –∫–ª–∏–∫–∞, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –Ω–µ —Å—Ä–∞–±–æ—Ç–∞—Ç—å, –±–µ—Ä–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏–Ω–≤–µ–Ω—Ç–∞—Ä—å –Ω–∞–ø—Ä—è–º—É—é –∏–∑ –∫–æ–¥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        soup = BeautifulSoup(await page.content(), 'html.parser')
        car_links = []
        for a in soup.find_all('a', href=True):
            href = a['href']
            # –°–æ–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–∞—Ä—Ç–æ—á–∫–∏ –º–∞—à–∏–Ω
            if '/inventory/' in href and href.count('/') > 4:
                car_links.append(href)

        links = list(set(car_links))
        print(f"üîé –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –º–∞—à–∏–Ω: {len(links)}")

        if not links:
            # –ï—Å–ª–∏ –ø–æ —Å—Å—ã–ª–∫–∞–º–∏ –Ω–µ –Ω–∞—à–ª–∏, –ø—Ä–æ–±—É–µ–º –∑–∞–π—Ç–∏ –≤ —Ä–∞–∑–¥–µ–ª –∏–Ω–≤–µ–Ω—Ç–∞—Ä—è
            await page.goto("https://rbautotrade.com/inventory/", wait_until="networkidle")
            await human_behavior(page)
            soup = BeautifulSoup(await page.content(), 'html.parser')
            links = list(set([a['href'] for a in soup.find_all('a', href=True) if '/inventory/' in a['href'] and a['href'].count('/') > 4]))
            print(f"üîé –ü–æ–≤—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –¥–∞–ª: {len(links)} –º–∞—à–∏–Ω")

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –º–∞—à–∏–Ω—ã
        for link in links[:15]: # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 15 —Å–≤–µ–∂–∏—Ö –ª–æ—Ç–æ–≤
            await scrape_car_details(context, link)
            await asyncio.sleep(random.uniform(2, 4)) # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ª–æ—Ç–∞–º–∏

        await browser.close()

if __name__ == "__main__":
    asyncio.run(run_parser())