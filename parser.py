import asyncio
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from supabase import create_client

# –î–∞–Ω–Ω—ã–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è (–≤—à–∏—Ç—ã –Ω–∞–ø—Ä—è–º—É—é –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏)
URL = "https://qenfvaqhbbqmorhvwmxg.supabase.co"
KEY = "sb_secret_JCr9KEjlFpMBO3fymu5l-w_AHPa7qac" 

supabase = create_client(URL, KEY)

async def scrape_details(context, url):
    page = await context.new_page()
    try:
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –∏ –∂–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–æ—Ç–æ
        await page.goto(url, wait_until="networkidle", timeout=60000)
        await asyncio.sleep(5) 
        
        soup = BeautifulSoup(await page.content(), 'html.parser')
        
        # –ü–æ–∏—Å–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        title_tag = soup.select_one('h1.entry-title') or soup.find('h1')
        title = title_tag.get_text(strip=True).upper() if title_tag else "UNKNOWN CAR"

        # –ü–æ–∏—Å–∫ —Ñ–æ—Ç–æ (–Ω–æ–≤—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è rbautotrade)
        img_tag = soup.select_one('img.wp-post-image') or soup.select_one('.elementor-image img')
        img_url = img_tag.get('src') if img_tag else ""
        
        if img_url and img_url.startswith('/'):
            img_url = "https://rbautotrade.com" + img_url

        car_data = {
            "external_id": url.split('/')[-2] if url.endswith('/') else url.split('/')[-1],
            "brand_model": title,
            "description": "Verified stock from Korea",
            "image_url": img_url,
            "source_url": url
        }
        
        # –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ –±–∞–∑—É
        supabase.table("cars").upsert(car_data).execute()
        print(f"‚úÖ –ó–∞–ø–∏—Å–∞–Ω–æ: {title}")

    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –Ω–∞ {url}: {e}")
    finally:
        await page.close()

async def run_parser():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(user_agent="Mozilla/5.0")
        page = await context.new_page()
        
        print("üîó –û—Ç–∫—Ä—ã–≤–∞–µ–º –∏–Ω–≤–µ–Ω—Ç–∞—Ä—å...")
        await page.goto("https://rbautotrade.com/inventory/", wait_until="networkidle")
        
        # –ñ–¥–µ–º, –ø–æ–∫–∞ –∑–∞–≥—Ä—É–∑—è—Ç—Å—è —Å—Å—ã–ª–∫–∏ –Ω–∞ –º–∞—à–∏–Ω—ã
        await page.wait_for_selector('a[href*="/inventory/"]')
        
        soup = BeautifulSoup(await page.content(), 'html.parser')
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏, –∏—Å–∫–ª—é—á–∞—è —Å–∞–º—É —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏–Ω–≤–µ–Ω—Ç–∞—Ä—è
        all_links = [a['href'] for a in soup.select('a[href*="/inventory/"]')]
        links = list(set([l for l in all_links if l.strip('/') != "https://rbautotrade.com/inventory"]))
        
        print(f"üîé –ù–∞–π–¥–µ–Ω–æ –º–∞—à–∏–Ω: {len(links)}")
        
        for link in links[:12]: # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 12 –¥–ª—è –Ω–∞–ø–æ–ª–Ω–µ–Ω–∏—è
            await scrape_details(context, link)
            
        await browser.close()

if __name__ == "__main__":
    asyncio.run(run_parser())