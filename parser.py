import os
import asyncio
import random
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from supabase import create_client

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Supabase: –±–µ—Ä–µ–º –∏–∑ —Å–µ–∫—Ä–µ—Ç–æ–≤ GitHub
URL = os.environ.get("SUPABASE_URL")
KEY = os.environ.get("SUPABASE_KEY")

# –ï—Å–ª–∏ –∑–∞–ø—É—Å–∫–∞–µ—à—å –ª–æ–∫–∞–ª—å–Ω–æ –Ω–∞ Mac, –ø–æ–¥—Å—Ç–∞–≤—å —Å–≤–æ–∏ —Å—Ç—Ä–æ–∫–∏ –∑–¥–µ—Å—å:
if not URL or not KEY:
    URL = "https://qenfvaqhbbqmorhvwmxg.supabase.co"
    KEY = "sb_secret_JCr9KEjlFpMBO3fymu5l-w_AHPa7qac"

supabase = create_client(URL, KEY)

async def scrape_details(context, url):
    page = await context.new_page()
    try:
        await page.goto(url, wait_until="domcontentloaded", timeout=60000)
        await asyncio.sleep(random.uniform(1, 3))
        soup = BeautifulSoup(await page.content(), 'html.parser')
        
        title_tag = soup.find(['h1', 'h4'])
        title = title_tag.get_text(strip=True).upper() if title_tag else "–ë–ï–ó –ù–ê–ó–í–ê–ù–ò–Ø"

        if "[SOLDOUT]" in title or "SOLD" in title:
            print(f"‚è© –ü–†–û–ü–£–°–ö: {title}")
            return

        description = soup.find('div', class_='entry-content')
        desc_text = description.get_text(strip=True)[:500] if description else ""
        imgs = [img['src'] for img in soup.find_all('img') if 'cdn' in img.get('src', '')]
        
        car_data = {
            "external_id": url.split('/')[-1].strip('/'),
            "brand_model": title,
            "description": desc_text,
            "image_url": imgs[0] if imgs else "",
            "source_url": url
        }
        
        supabase.table("cars").upsert(car_data).execute()
        print(f"‚úÖ –î–û–ë–ê–í–õ–ï–ù–û: {title}")

    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –Ω–∞ {url}: {e}")
    finally:
        await page.close()

async def run_parser():
    async with async_playwright() as p:
        # –ì–õ–ê–í–ù–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: headless=True (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–ª—è GitHub)
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context()
        page = await context.new_page()

        print("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ –≤ –æ–±–ª–∞–∫–µ...")
        await page.goto("https://rbautotrade.com/inventory/", wait_until="networkidle")

        links = [a['href'] for a in (BeautifulSoup(await page.content(), 'html.parser')).select('a[href*="/inventory/"]')]
        unique_links = list(set(links))
        print(f"üîé –ù–∞–π–¥–µ–Ω–æ –∞–≤—Ç–æ: {len(unique_links)}")

        for link in unique_links:
            await scrape_details(context, link)

        await browser.close()
        print("üéâ –ì–æ—Ç–æ–≤–æ!")

if __name__ == "__main__":
    asyncio.run(run_parser())