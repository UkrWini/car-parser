import requests
from bs4 import BeautifulSoup
from supabase import create_client
import time

# –¢–≤–æ–∏ –¥–∞–Ω–Ω—ã–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
URL = "https://qenfvaqhbbqmorhvwmxg.supabase.co"
KEY = "sb_secret_JCr9KEjlFpMBO3fymu5l-w_AHPa7qac" 
supabase = create_client(URL, KEY)

def scrape():
    print("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ —á–µ—Ä–µ–∑ –æ–±—Ö–æ–¥–Ω–æ–π –ø—É—Ç—å...")
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–µ—Ä–≤–∏—Å-–ø—Ä–æ–∫–ª–∞–¥–∫—É, —á—Ç–æ–±—ã —Å–∫—Ä—ã—Ç—å IP –ì–∏—Ç—Ö–∞–±–∞
    # –≠—Ç–æ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –∞–Ω–æ–Ω–∏–º–∞–π–∑–µ—Ä
    proxy_url = "https://api.allorigins.win/get?url="
    target_url = "https://rbautotrade.com/inventory/"
    
    try:
        response = requests.get(f"{proxy_url}{target_url}", timeout=30)
        data = response.json()
        html = data['contents']
        
        soup = BeautifulSoup(html, 'html.parser')
        links = []
        for a in soup.find_all('a', href=True):
            href = a['href']
            if '/inventory/' in href and len(href) > 35:
                links.append(href)
        
        unique_links = list(set(links))
        print(f"üîé –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–æ—Ç–æ–≤: {len(unique_links)}")

        if not unique_links:
            print("‚ö†Ô∏è –°—Å—ã–ª–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –¥–∞–∂–µ —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏. –ü—Ä–æ–±—É—é –º–µ—Ç–æ–¥ '–ñ–∞—Ç–≤–∞'.")
            return

        for link in unique_links[:10]:
            print(f"üïµÔ∏è –ö–∞—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ: {link}")
            car_res = requests.get(f"{proxy_url}{link}", timeout=30)
            car_html = car_res.json()['contents']
            car_soup = BeautifulSoup(car_html, 'html.parser')
            
            title = car_soup.find('h1').get_text(strip=True).upper() if car_soup.find('h1') else "KOREA CAR"
            
            # –ö–∞—Ä—Ç–∏–Ω–∫–∏
            images = [img['src'] for img in car_soup.find_all('img', src=True) if 'uploads' in img['src']]
            main_img = images[0] if images else ""
            
            # –û–ø–∏—Å–∞–Ω–∏–µ
            desc = car_soup.select_one('.entry-content').get_text(strip=True) if car_soup.select_one('.entry-content') else "No description"

            car_data = {
                "external_id": link.strip('/').split('/')[-1],
                "brand_model": title,
                "description": desc[:1000],
                "image_url": main_img,
                "source_url": link
            }

            supabase.table("cars").upsert(car_data).execute()
            print(f"‚úÖ –ó–∞–ø–∏—Å–∞–Ω–æ –≤ –±–∞–∑—É: {title}")
            time.sleep(2)

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    scrape()
    