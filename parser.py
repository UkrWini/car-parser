import asyncio
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from supabase import create_client

# –î–∞–Ω–Ω—ã–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
URL = "https://qenfvaqhbbqmorhvwmxg.supabase.co"
KEY = "sb_secret_JCr9KEjlFpMBO3fymu5l-w_AHPa7qac" # –¢–≤–æ–π Secret Key

supabase = create_client(URL, KEY)

async def scrape_details(context, url):
    page = await context.new_page()
    try:
        # –ó–∞—Ö–æ–¥–∏–º –∏ –ø—Ä–æ—Å—Ç–æ –∂–¥—ë–º 5 —Å–µ–∫—É–Ω–¥, –Ω–µ –ø—ã—Ç–∞—è—Å—å –∏—Å–∫–∞—Ç—å —Å–µ–ª–µ–∫—Ç–æ—Ä—ã —Å—Ä–∞–∑—É
        await page.goto(url, wait_until="domcontentloaded", timeout=60000)
        await asyncio.sleep(5) 
        
        soup = BeautifulSoup(await page.content(), 'html.parser')
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        title_tag = soup.select_one('h1.entry-title') or soup.find('h1')
        title = title_tag.get_text(strip=True).upper() if title_tag else "UNKNOWN CAR"

        # –ö–∞—Ä—Ç–∏–Ω–∫–∞ (–∏—â–µ–º –≤–µ–∑–¥–µ, –≥–¥–µ –º–æ–∂–µ—Ç –±—ã—Ç—å)
        img_tag = soup.select_one('img.wp-post-image') or soup.select_one('.elementor-image img') or soup.find('img', src=lambda x: x and 'uploads' in x)
        img_url = img_tag.get('src') if img_tag else ""

        car_data = {
            "external_id": url.strip('/').split('/')[-1],
            "brand_model": title,
            "description": "Stock from Korea",
            "image_url": img_url,
            "source_url": url
        }
        
        supabase.table("cars").upsert(car_data).execute()
        print(f"‚úÖ –ì–æ—Ç–æ–≤–æ: {title}")

    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –Ω–∞ {url}: {e}")
    finally:
        await page.close()

async def run_parser():
    async with async_playwright() as p:
        # –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê–Ø –ú–ê–°–ö–ò–†–û–í–ö–ê
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            viewport={'width': 1920, 'height': 1080}
        )
        page = await context.new_page()
        
        print("üîó –ü—Ä–æ–±—É–µ–º –ø—Ä–æ–±–∏—Ç—å—Å—è –Ω–∞ —Å–∞–π—Ç...")
        await page.goto("https://rbautotrade.com/inventory/", wait_until="domcontentloaded")
        await asyncio.sleep(7) # –î–∞—ë–º —Å–∞–π—Ç—É ¬´–ø—Ä–æ–≥—Ä—É–∑–∏—Ç—å—Å—è¬ª
        
        soup = BeautifulSoup(await page.content(), 'html.parser')
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ –≤—Ä—É—á–Ω—É—é, —á—Ç–æ–±—ã –Ω–µ –∑–∞–≤–∏—Å–µ—Ç—å –æ—Ç Playwright selectors
        links = []
        for a in soup.find_all('a', href=True):
            if '/inventory/' in a['href'] and a['href'] != "https://rbautotrade.com/inventory/":
                links.append(a['href'])
        
        unique_links = list(set(links))
        print(f"üîé –ù–∞–π–¥–µ–Ω–æ –º–∞—à–∏–Ω: {len(unique_links)}")
        
        for link in unique_links[:10]: # –ë–µ—Ä—ë–º 10 –¥–ª—è —Ç–µ—Å—Ç–∞
            await scrape_details(context, link)
            
        await browser.close()

if __name__ == "__main__":
    asyncio.run(run_parser())